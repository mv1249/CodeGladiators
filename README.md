# CodeGladiators
 
## Problem Statement

<div class="content-block-extra">
    <p></p>
    <div>Digital Advertising is changing at a rapid pace with a huge increase in digital audience. At the same time, the digital advertising success metric is shifting from audience volume (eg. Impression count) to conversions (eg. lead submissions) as the
        success metric. This requires higher transparency and control on the conversions.</div>
    <div><br></div>
    <div>Colombia, the digital advertising arm of Times Internet Limited has seen significant growth in its digital advertising inventory. It wants to ensure that in all its conversion-based campaigns, no unfair advantage is given to the publishers generating
        fake leads. </div>
    <div><br></div>
    <div><b>Your task is to segregate the test data between genuine and false conversions by identifying the maximum possible 
            leads generated by the malignant technique.</b></div>
    <div><br></div>
    <div><b><u>Note- </u></b></div>
    <div>Joining with Click log: </div>
    <div>
        <div>imprId (Click Log) and imprid_cr (Conversion Log - Test and Train Data)</div>
        <div><br></div>
        <div>
            (Use click log data for additional data required for identifying conversion fraud)</div>
        <div><br></div>
        <div><b>Essential Columns</b>
            < /div>
                <div>
                    <ul>
                        <li>client id: Advertiser ID</li>
                        <li>pubclient id: Publisher ID</li>
                        <li>clickIp: IP Address</li>
                        <li>clmbuser id : unique user id</li>
                        <li>
                            impr id: Unique Key for every served impression</li>
                        <li>site id: Publisher wesite</li>
                        <li>goal id: Conversion`s goal type identification id</li>
                        <li>
                            City id / State id / CountryDim id: Geo Details</li>
                        <li>browser id: browser used for accessing publisher on any device on web.</li>
                        <li>adslot id: slot id where advertisement is displayed on any site (unqiue for all sites)</li>
                        <li>crtd: timestamp of the action</li>
                        <li>itmclmb id: Image/Creative shown</li>
                        <li>
                            ispDimId: Internet Service Provider</li>
                        <li>devTypeDimId: Device Id</li>
                        <li>osVerDimId: OS Version</li>
                    </ul>
                </div>
        </div>
    </div>
</div>

## Download the Dataset from here
<h4>Data Set <a href="https://www.techgig.com/files/DataScienceFullData/326708/AllDataSet.zip" class="action-lnk">Download Data Set</a></h4>

## My Approach 

<ol>
  <liFirst Approach was to join all the necessary columns and form a single csv file which contains all the data in one file.</li>

  <li>Second as the Dataset was imbalanced, so we have to balance the dataset by using undersampling ,oversampling or Smote.</li>


  <li>After that it is the turn to perform Exploratory Data Analysis on the dataset, generating insights and facts</li>


  <li>After that we check Transformations of the columns and try to fit the data in a Gaussian Distribution using Various Techniques,</li>


  <li>After Transformations its time for Feature Selection followed by Model Building and Hyperparameter Tuning.</li>

  <li>Last step is to work on metrics and then choose the appropriate model</li>
  
  <li>Then I moved on Model building and Hyperparameter Tuning and Evaluation of Metrics and then Rechecking them with the FAST AI Model.</li>

</ol>

## Observations


After possibly fixing all the transformations,using the Box-cox Transformation,i moved for outlier Detection and Found Outliers in Two columns,
and the best possible way to fill in values of those outliers was by filling with the min-max startegy,here is the image corresponding to it,we can observe that
either Mode or Median is a better choice for filling in the outliers for the "Client ID" column

<p align = "center">
   <img src="https://github.com/mv1249/CodeGladiators/blob/main/images/5.PNG">
</p>

After fixing all the transformations and outliers,i went for Modelling where i started with Ensemble Techniques,The First thing which i did was to get the value of 
`ccp_alpha`,a hyperparamter which helps in pruning the Decision Tree so that we can avoid Overfitting,I could have used Bagging Technique as well
Here is the Graph for the Training and Testing Curves,which state the nature of the Decision Tree Algorithm

<p align = "center">
   <img src="https://github.com/mv1249/CodeGladiators/blob/main/images/2.PNG">
</p>

From the above graph we observe that if we set the ccp_alpha values in between [0.005,0.015] we will be getting a better version of the Decision Tree,here is one sample where
i have set the value of `ccp_alpha` to be 0.015 and this is the resulted tree,previously the tree was very huge and its depth was almost till 20 levels!,after pruning it reduced

<p align = "center">
   <img src="https://github.com/mv1249/CodeGladiators/blob/main/images/3.PNG">
</p>

After this i moved on with Feature Selection Using the Mutual Information Gain and ExtraTrees Classifier and identified the features which gave me most information among
all the features,here is the feature contribution and its value

<p align = "center">
   <img src="https://github.com/mv1249/CodeGladiators/blob/main/images/4.PNG">
</p>

After selecting the best features,then i moved on for model Building and Hyperparameter Tuning and Then Predictive Modelling using `FAST AI`,here is the image of the 
`value_counts` of the `conversion_fraud` column,

<p align = "center">
   <img src="">
</p>
